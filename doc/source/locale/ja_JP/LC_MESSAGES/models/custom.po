# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-07-09 14:56+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: ja_JP\n"
"Language-Team: ja_JP <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/models/custom.rst:5
msgid "Custom Models"
msgstr ""

#: ../../source/models/custom.rst:6
msgid ""
"Xinference provides a flexible and comprehensive way to integrate, "
"manage, and utilize custom models."
msgstr ""

#: ../../source/models/custom.rst:9
msgid "Define a custom LLM model"
msgstr ""

#: ../../source/models/custom.rst:11
msgid "Define a custom LLM model based on the following template:"
msgstr ""

#: ../../source/models/custom.rst:52 ../../source/models/custom.rst:181
#: ../../source/models/custom.rst:204
msgid ""
"model_name: A string defining the name of the model. The name must start "
"with a letter or a digit and can only contain letters, digits, "
"underscores, or dashes."
msgstr ""

#: ../../source/models/custom.rst:53
msgid ""
"context_length: context_length: An optional integer that specifies the "
"maximum context size the model was trained to accommodate, encompassing "
"both the input and output lengths. If not defined, the default value is "
"2048 tokens (~1,500 words)."
msgstr ""

#: ../../source/models/custom.rst:54
msgid ""
"model_lang: A list of strings representing the supported languages for "
"the model. Example: [\"en\"], which means that the model supports "
"English."
msgstr ""

#: ../../source/models/custom.rst:55
msgid ""
"model_ability: A list of strings defining the abilities of the model. It "
"could include options like \"embed\", \"generate\", and \"chat\". In this"
" case, the model has the ability to \"generate\"."
msgstr ""

#: ../../source/models/custom.rst:56
msgid ""
"model_family: A required string representing the family of the model you "
"want to register. The optional values are the model names of all :ref"
":`built-in models <models_llm_index>`. If the model family you register "
"is not among the built-in models in Xinference, please fill in ``other``."
" Note that you should choose the model family based on the ability of the"
" model you want to register. For example, if you want to register the "
"``llama-2`` model, do not fill in ``llama-2-chat`` as the model family."
msgstr ""

#: ../../source/models/custom.rst:62
msgid ""
"model_specs: An array of objects defining the specifications of the "
"model. These include:"
msgstr ""

#: ../../source/models/custom.rst:58
msgid ""
"model_format: A string that defines the model format, could be "
"\"pytorch\" or \"ggmlv3\"."
msgstr ""

#: ../../source/models/custom.rst:59
msgid ""
"model_size_in_billions: An integer defining the size of the model in "
"billions of parameters."
msgstr ""

#: ../../source/models/custom.rst:60
msgid ""
"quantizations: A list of strings defining the available quantizations for"
" the model. For PyTorch models, it could be \"4-bit\", \"8-bit\", or "
"\"none\". For ggmlv3 models, the quantizations should correspond to "
"values that work with the ``model_file_name_template``."
msgstr ""

#: ../../source/models/custom.rst:61
msgid ""
"model_id: A string representing the model ID, possibly referring to an "
"identifier used by Hugging Face. **If model_uri is missing, Xinference "
"will try to download the model from the huggingface repository specified "
"here.**."
msgstr ""

#: ../../source/models/custom.rst:62
msgid ""
"model_uri: A string representing the URI where the model can be loaded "
"from, such as \"file:///path/to/llama-2-7b\". **When the model format is "
"ggmlv3 or ggufv2, model_uri must be the specific file path. When the "
"model format is pytorch, model_uri must be the path to the directory "
"containing the model files.** If model URI is absent, Xinference will try"
" to download the model from Hugging Face with the model ID."
msgstr ""

#: ../../source/models/custom.rst:63
msgid ""
"model_file_name_template: Required by ggml/gguf models. An f-string "
"template used for defining the model file name based on the quantization."
" **Note that this field is just a template for the format of the "
"ggmlv3/ggufv2 model file, do not fill in the specific path of the model "
"file.**"
msgstr ""

#: ../../source/models/custom.rst:64
msgid ""
"prompt_style: If the ``model_family`` field is not ``other``, this field "
"does not need to be filled in. ``prompt_style`` is an optional field that"
" could be required by ``chat`` models to define the style of prompts. The"
" given example has this set to None, but additional details could be "
"found in a referenced file xinference/model/llm/tests/test_utils.py. You "
"can also specify this field as a string, which will use the builtin "
"prompt style in Xinference. For example:"
msgstr ""

#: ../../source/models/custom.rst:73
msgid "Xinference supports these builtin prompt styles in common usage:"
msgstr ""

#: ../../source/models/custom.rst:77
msgid "baichuan-chat"
msgstr ""

#: ../../source/models/custom.rst:96
msgid "chatglm3"
msgstr ""

#: ../../source/models/custom.rst:109
msgid "qwen-chat"
msgstr ""

#: ../../source/models/custom.rst:126
msgid "llama-2-chat"
msgstr ""

#: ../../source/models/custom.rst:147
msgid "vicuna-v1.5"
msgstr ""

#: ../../source/models/custom.rst:162
msgid ""
"The above lists some commonly used built-in prompt styles. The full list "
"of supported prompt styles can be found on the Xinference web UI."
msgstr ""

#: ../../source/models/custom.rst:166
msgid "Define a custom embedding model"
msgstr ""

#: ../../source/models/custom.rst:168
msgid "Define a custom embedding model based on the following template:"
msgstr ""

#: ../../source/models/custom.rst:182
msgid "dimensions: A integer that specifies the embedding dimensions."
msgstr ""

#: ../../source/models/custom.rst:183
msgid ""
"max_tokens: A integer that represents the max sequence length that the "
"embedding model supports."
msgstr ""

#: ../../source/models/custom.rst:184 ../../source/models/custom.rst:206
msgid ""
"language: A list of strings representing the supported languages for the "
"model. Example: [\"en\"], which means that the model supports English."
msgstr ""

#: ../../source/models/custom.rst:185 ../../source/models/custom.rst:207
msgid ""
"model_id: A string representing the model ID, possibly referring to an "
"identifier used by Hugging Face."
msgstr ""

#: ../../source/models/custom.rst:186 ../../source/models/custom.rst:208
msgid ""
"model_uri: A string representing the URI where the model can be loaded "
"from, such as \"file:///path/to/your_model\". If model URI is absent, "
"Xinference will try to download the model from Hugging Face with the "
"model ID."
msgstr ""

#: ../../source/models/custom.rst:190
msgid "Define a custom Rerank model"
msgstr ""

#: ../../source/models/custom.rst:192
msgid "Define a custom rerank model based on the following template:"
msgstr ""

#: ../../source/models/custom.rst:205
msgid ""
"type: A string defining the type of the model, including ``normal``, "
"``LLM-based`` and ``LLM-based layerwise``."
msgstr ""

#: ../../source/models/custom.rst:212
msgid "Register a Custom Model"
msgstr ""

#: ../../source/models/custom.rst:214
msgid "Register a custom model programmatically:"
msgstr ""

#: ../../source/models/custom.rst:229 ../../source/models/custom.rst:247
#: ../../source/models/custom.rst:262 ../../source/models/custom.rst:317
msgid "Or via CLI:"
msgstr ""

#: ../../source/models/custom.rst:235
msgid ""
"Note that replace the ``<model_type>`` above with ``LLM``, ``embedding`` "
"or ``rerank``. The same as below."
msgstr ""

#: ../../source/models/custom.rst:239
msgid "List the Built-in and Custom Models"
msgstr ""

#: ../../source/models/custom.rst:241
msgid "List built-in and custom models programmatically:"
msgstr ""

#: ../../source/models/custom.rst:254
msgid "Launch the Custom Model"
msgstr ""

#: ../../source/models/custom.rst:256
msgid "Launch the custom model programmatically:"
msgstr ""

#: ../../source/models/custom.rst:269
msgid "Interact with the Custom Model"
msgstr ""

#: ../../source/models/custom.rst:271
msgid "Invoke the model programmatically:"
msgstr ""

#: ../../source/models/custom.rst:278
msgid "Result:"
msgstr ""

#: ../../source/models/custom.rst:302
msgid "Or via CLI, replace ``${UID}`` with real model UID:"
msgstr ""

#: ../../source/models/custom.rst:309
msgid "Unregister the Custom Model"
msgstr ""

#: ../../source/models/custom.rst:311
msgid "Unregister the custom model programmatically:"
msgstr ""

#~ msgid "Define a custom model"
#~ msgstr ""

#~ msgid "Define a custom model based on the following template:"
#~ msgstr ""

#~ msgid ""
#~ "model_uri: A string representing the URI"
#~ " where the model can be loaded "
#~ "from, such as \"file:///path/to/llama-2-7b\". "
#~ "If model URI is absent, Xinference "
#~ "will try to download the model "
#~ "from Hugging Face with the model "
#~ "ID."
#~ msgstr ""

#~ msgid ""
#~ "model_file_name_template: Required by ggml "
#~ "models. An f-string template used for"
#~ " defining the model file name based"
#~ " on the quantization."
#~ msgstr ""

#~ msgid ""
#~ "prompt_style: An optional field that "
#~ "could be required by chat models "
#~ "to define the style of prompts. "
#~ "The given example has this set to"
#~ " None, but additional details could "
#~ "be found in a referenced file "
#~ "xinference/model/llm/tests/test_utils.py."
#~ msgstr ""

#~ msgid ""
#~ "prompt_style: An optional field that "
#~ "could be required by chat models "
#~ "to define the style of prompts. "
#~ "The given example has this set to"
#~ " None, but additional details could "
#~ "be found in a referenced file "
#~ "xinference/model/llm/tests/test_utils.py. You can "
#~ "also specify this field as a "
#~ "string, which will use the builtin "
#~ "prompt style in Xinference. For example:"
#~ msgstr ""

#~ msgid ""
#~ "Note that replace the ``<model_type>`` "
#~ "above with ``LLM`` or ``embedding``. The"
#~ " same as below."
#~ msgstr ""

#~ msgid ""
#~ "Define a custom rerank model based "
#~ "on the following template: .. code-"
#~ "block:: json"
#~ msgstr ""

#~ msgid "{"
#~ msgstr ""

#~ msgid ""
#~ "\"model_name\": \"custom-bge-reranker-v2-m3\", "
#~ "\"type\": \"normal\", \"language\": [\"en\", "
#~ "\"zh\", \"multilingual\"], \"model_id\": \"BAAI"
#~ "/bge-reranker-v2-m3\", \"model_uri\": \"file:///path/to"
#~ "/bge-reranker-v2-m3\""
#~ msgstr ""

#~ msgid "}"
#~ msgstr ""

